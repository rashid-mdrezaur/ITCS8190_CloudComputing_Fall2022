{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CuxpYhhC3r0",
    "outputId": "366a72bd-a25b-4921-e5d5-d7b4a14d82cb"
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qah6L0EC9-L",
    "outputId": "2892a479-c737-4d57-c1c5-77b94da9fdff"
   },
   "outputs": [],
   "source": [
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "id": "2dchrER6C_4Y"
   },
   "outputs": [],
   "source": [
    "## initialize all the libraries\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix, BlockMatrix, DenseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "id": "jOCSaznqDDls"
   },
   "outputs": [],
   "source": [
    "# initiate the spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('yarn') \\\n",
    "    .appName('matrix-multiplication') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "id": "IoANMXyfDI2g"
   },
   "outputs": [],
   "source": [
    "## RDD multiplication using indexedMatrix for efficiency of larger dataset\n",
    "def RDD_multiply(my_arr, weight):\n",
    "    # print(my_arr.ndim)\n",
    "    row_list = []\n",
    "    if my_arr.ndim == 1:\n",
    "        # print('dim: ', my_arr.ndim)\n",
    "        row_list.append((0, tuple(my_arr)))\n",
    "    else:\n",
    "        for i in range(my_arr.shape[0]):\n",
    "            tup_elem = tuple(my_arr[i])\n",
    "            tup_rows = (i,tup_elem)\n",
    "            row_list.append(tup_rows)\n",
    "      \n",
    "    indMat = IndexedRowMatrix(sc.parallelize(row_list))\n",
    "\n",
    "    denMat = DenseMatrix(weight.shape[0], weight.shape[1], weight.T.flatten())\n",
    "    mulMat = indMat.multiply(denMat)\n",
    "    new_m = mulMat.toBlockMatrix().toLocalMatrix().toArray().astype('float64')\n",
    "\n",
    "    return new_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "id": "g54MaDhAFbk_"
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] \n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "id": "b9OKsgS6c_XZ"
   },
   "outputs": [],
   "source": [
    "## calculating the multi-class SVM loss\n",
    "\n",
    "def svm_loss(W, X, y, reg, delta=1.0):\n",
    "    \n",
    "    # initializing the gradient as zero\n",
    "    \n",
    "    dW = np.zeros(W.shape) \n",
    "\n",
    "    # computing the loss and the gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(num_train):\n",
    "        y_ = y[i]\n",
    "\n",
    "        scores = X[i].dot(W)\n",
    "        # scores = RDD_multiply(X[i],W)\n",
    "        correct_class_score = scores[y_]\n",
    "        for j in range(num_classes):\n",
    "            if j == y_:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + delta\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:,j]  += X[i]\n",
    "                dW[:,y_] -= X[i]\n",
    "\n",
    "   #average\n",
    "    loss /= float(num_train)\n",
    "    dW   /= float(num_train)\n",
    "\n",
    "   # Adding a L2 regularization to the loss.\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW   += reg * W\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "id": "kILqnQIBlkdR"
   },
   "outputs": [],
   "source": [
    "## SVM classifier\n",
    "def svmPredict(X, Y, reg_param, tts = 70):\n",
    "\n",
    "    num_class = len(np.unique(Y))\n",
    "    \n",
    "    # first insert 1 in every row for intercept b\n",
    "    X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "\n",
    "    Xx = X.to_numpy()\n",
    "    Yy = Y\n",
    "\n",
    "    training_percentage = tts\n",
    "    size_of_input_data = X.shape[0]\n",
    "\n",
    "    Xtr = Xx[0 : (int(size_of_input_data*training_percentage/100)), :]\n",
    "    Ytr = Yy[0: (int(size_of_input_data*training_percentage/100))]\n",
    "    Xte = Xx[(int(size_of_input_data*training_percentage/100)) : , :]\n",
    "    Yte = Yy[(int(size_of_input_data*training_percentage/100)) : ]\n",
    "\n",
    "    W = np.zeros((Xtr.shape[1], num_class))\n",
    "\n",
    "    loss, dW = svm_loss(W, Xtr, Ytr, reg_param)\n",
    "    # print(dW)\n",
    "\n",
    "    # Z = -Xte@dW\n",
    "    # print(Z)\n",
    "    Z = RDD_multiply(Xte, dW)\n",
    "    Z = -Z\n",
    "    # print(Z)\n",
    "    P = softmax(Z)\n",
    "    pred = np.argmax(P, axis=1)\n",
    "    \n",
    "    acc = (pred == Yte).sum()/Yte.shape[0]\n",
    "    \n",
    "    return acc, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRfJHVVFmyDQ",
    "outputId": "dc479581-9498-4df8-e659-b7939dd3e0df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## data reading\n",
    "\n",
    "# df = spark.read.csv('S3://my_cloud2022_bucket/airlines_tweets_cleaned.csv', header=True)   ## use (uncomment) this line if dataset is from AWS S3 bucket\n",
    "df = spark.read.csv('data/airlines_tweets_cleaned.csv', header=True) ##use this line if read the dataset from local directory\n",
    "df.count()\n",
    "df = df.select('text', 'target')\n",
    "df = df.dropna()\n",
    "df.count()\n",
    "# df.show(5)\n",
    "\n",
    "df.printSchema()\n",
    "panda_df = df.toPandas()\n",
    "sentence_list = list(panda_df['text'])\n",
    "word_lists = []\n",
    "word_token = []\n",
    "word_dict = {}\n",
    "count = 1\n",
    "\n",
    "for i in range(len(sentence_list)):\n",
    "    my_sent = str(sentence_list[i])\n",
    "    # print(type(my_sent))\n",
    "    words = my_sent.split(' ')\n",
    "    word_token.append(words)\n",
    "    for w in words:\n",
    "        if w not in word_lists:\n",
    "            word_lists.append(w)\n",
    "            word_dict[w]=count\n",
    "            count = count+1\n",
    "\n",
    "input_x = np.zeros((len(panda_df), 100))\n",
    "\n",
    "for tok in range(len(word_token)):\n",
    "    my_token = word_token[tok]\n",
    "\n",
    "    start = (100-len(my_token))\n",
    "    for i in range(len(my_token)):\n",
    "        input_x[tok,start] = word_dict[my_token[i]]\n",
    "        start = start+1\n",
    "new_df = pd.DataFrame(input_x)\n",
    "new_df = new_df.loc[:, (new_df != 0).any(axis=0)]\n",
    "norm_x = (new_df - new_df.min())/(new_df.max() - new_df.min())\n",
    "y_label = np.array(panda_df['target']).astype(int)\n",
    "data = norm_x.copy()\n",
    "data['target'] = y_label\n",
    "data_df = spark.createDataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B32-q6VUm9g-",
    "outputId": "ce94cce6-d804-4d91-9256-959c2a9c2baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass SVM Testing Accuracy: 72.83 %\n",
      "Total number of words: 11129\n",
      "Important words: 6057\n"
     ]
    }
   ],
   "source": [
    "reg_param = .1\n",
    "acc, model_w = svmPredict(norm_x, y_label, reg_param)\n",
    "print('Multiclass SVM Testing Accuracy:', round((acc*100),2), '%')\n",
    "\n",
    "\n",
    "## get_imp_words list\n",
    "my_w = np.abs(model_w).sum(axis=1)\n",
    "my_w = my_w[:(len(my_w)-1)]\n",
    "sorted_w = np.sort(my_w)\n",
    "top_5 = sorted_w[-3:]\n",
    "col_ind = []\n",
    "for i in top_5:\n",
    "    ind = np.where(my_w == i)\n",
    "    col_ind.append(ind[0][0])\n",
    "imp_col = new_df.iloc[:, col_ind]\n",
    "imp_word_index = np.array(imp_col).flatten()\n",
    "imp_word_index\n",
    "\n",
    "imp_word_index = imp_word_index[imp_word_index != 0]\n",
    "imp_word_index = np.unique(imp_word_index)\n",
    "num_imp_words = len(imp_word_index)\n",
    "num_total_words = len(word_dict)\n",
    "def get_key(my_dict,val):\n",
    "    for key, value in my_dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "imp_words_list = []\n",
    "for i in imp_word_index:\n",
    "    getword = get_key(word_dict,i)\n",
    "    imp_words_list.append(getword)\n",
    "\n",
    "with open(\"data/imp_words.txt\", \"w\") as output:\n",
    "    output.write(str(imp_words_list))\n",
    "print('Total number of words:', num_total_words)\n",
    "print('Important words:', num_imp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "id": "vBuQnt_XFY8M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
